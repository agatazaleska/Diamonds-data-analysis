{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agata Załęska - Project assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary analysis and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "\n",
    "path = './'\n",
    "data = pd.read_csv(f\"{path}/messy_data.csv\")\n",
    "\n",
    "print(f\"Data columns are:\\n {data.columns}\\n\")\n",
    "print(f\"{data.head(3)} \\n\")\n",
    "print(f\"Data size: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete leading and trailing spaces from columns names\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "print(f\"Adjusted data columns are:\\n {data.columns}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take care of duplicates and missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting duplicates\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# check if data is consistent\n",
    "# we see that it is not - only carat has always float value\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the data consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert other columns to correct types\n",
    "data['price'] = pd.to_numeric(data['price'], errors='coerce')\n",
    "data['carat'] = pd.to_numeric(data['carat'], errors='coerce')\n",
    "data['x dimension'] = pd.to_numeric(data['x dimension'], errors='coerce')\n",
    "data['y dimension'] = pd.to_numeric(data['y dimension'], errors='coerce')\n",
    "data['z dimension'] = pd.to_numeric(data['z dimension'], errors='coerce')\n",
    "data['depth'] = pd.to_numeric(data['depth'], errors='coerce')\n",
    "data['table'] = pd.to_numeric(data['table'], errors='coerce')\n",
    "\n",
    "\n",
    "data['clarity'] = data['clarity'].astype(str)\n",
    "data['color'] = data['color'].astype(str)\n",
    "data['cut'] = data['cut'].astype(str)\n",
    "\n",
    "print(data.dtypes)\n",
    "\n",
    "# change the spelling of cut, color and clarity to lower\n",
    "# to ensure that the values represented as text are consistent\n",
    "data['clarity'] = data['clarity'].str.lower()\n",
    "data['color'] = data['color'].str.lower()\n",
    "data['cut'] = data['cut'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check null values and delete rows with null price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the numebr of null values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# delete the rows with null price - price is our dependent variable vector\n",
    "data = data.dropna(subset=['price'])\n",
    "\n",
    "# filter out the data where prices are extreme\n",
    "lower_bound, upper_bound = data['price'].quantile(0.05), data['price'].quantile(0.95)\n",
    "data = data[(data['price'] >= lower_bound) & (data['price'] <= upper_bound)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create clean data file for the dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"clean_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill the missing feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the null numeric values with the mean of the column values\n",
    "# we do this only for the features\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "replace_cols = ['carat', 'x dimension', 'y dimension', 'z dimension', 'depth', 'table']\n",
    "imputer.fit(data[replace_cols])\n",
    "data[replace_cols] = imputer.transform(data[replace_cols]) # replace the missing values\n",
    "\n",
    "print(f\"{data.head(3)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_vars = ['clarity', 'color', 'cut']\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"encoder\", encoder, categorical_vars)], remainder='passthrough')\n",
    "\n",
    "encoded_data = transformer.fit_transform(data)\n",
    "\n",
    "encoded_columns = transformer.get_feature_names_out()[:-7] # do not take the remainders\n",
    "remainder_columns = [col for col in data.columns if col not in categorical_vars]\n",
    "new_columns = np.append(encoded_columns, remainder_columns)\n",
    "\n",
    "# create new dataframe to maintain meaningful column names\n",
    "encoded_data = pd.DataFrame(encoded_data, columns=new_columns)\n",
    "print(encoded_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the feature matrix and dependent variable vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = encoded_data.drop('price', axis=1)\n",
    "y = encoded_data['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def backward_elimination(data, target, significance_level=0.05):\n",
    "    features = data.columns.tolist()\n",
    "    while len(features) > 0:\n",
    "        features_with_constant = sm.add_constant(data[features])\n",
    "        p_values = sm.OLS(target, features_with_constant).fit().pvalues[1:]\n",
    "        max_p_value = p_values.max()\n",
    "        if max_p_value > significance_level:\n",
    "            excluded_feature = p_values.idxmax()\n",
    "            features.remove(excluded_feature)\n",
    "        else:\n",
    "            break\n",
    "    return features\n",
    "\n",
    "selected_features = backward_elimination(X, y)\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from joblib import dump\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# divide the data into test and train\n",
    "X_selected = X[selected_features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=0)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "dump(model, 'model.joblib')\n",
    "dump(X_selected, 'input_data.joblib')\n",
    "\n",
    "# measure the model accuracy on test data\n",
    "y_pred = model.predict(X_test)\n",
    "print('R^2:', r2_score(y_test, y_pred))\n",
    "\n",
    "# show the models effect on a plot\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('Rzeczywiste ceny')\n",
    "plt.ylabel('Przewidywane ceny')\n",
    "plt.title('Rzeczywiste vs Przewidywane ceny')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_pred), max(y_pred)], color='red')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
